# Reti neurali

> Domare gli attrattori

Qui entra l'approccio discusso all'inizio del corso. Le tendenze sono mappabili con attrattori.

## Esempi d'uso

OCR: riconoscere testo da una scansione.

## Bioispiriamoci: il cervello

Un computer è molto veloce, manipola grandi quantità di dati con operazioni fisse e in modo deterministico.

Il vantaggio è che il computer è una macchina di Turing, un simulatore generale. Possiamo fare degli algoritmi.
Se troviamo un algoritmo che simula un cervello, il computer è in grado di fare quello che facciamo noi.
Una rete neurale è una simulazione delle parti essenziali.

Se un neurone è molto stimolato manda fuori il suo segnale, altrimenti no. Quindi fa una somma e poi lo fa passare per
una soglia. Il tempo tipico di elaborazione degli stimoli è 1ms.

Ogni volta che vediamo qualcosa non si accende un solo neurone. Non è che solo un gruppettino rappresenta qualcosa di
particolare, ma c'è un'attivazione distribuita nel cervello. Quindi parti diverse del cervello agiscono in sincrono
per poter elaborare oppure memorizzare i dati.

Esistono anche i neuroni specchio; ovvero neuroni che attivano aree del cervello quando interpretiamo alcune azioni.
Ad esempio per capire se qualcuno ci sta ascoltando penso a cosa farei io se stessi ascoltando qualcuno.

Siamo anche robusti a fallimenti di neuroni.

## Modello booleano

Di McCulloch e Pitts

Primo modello di neurone simulato, rete neurale.
Il modello che usano è molto semplice: ha tanti link che gli arrivano, se la somma è sotto una certa soglia $\theta$ lui
emette segnale. I livelli sono sempre 2: 0 e 1, o -1 e 1.

A ogni link in ingresso è associato un peso, che determina l'intesità del segnale in arrivo.

L'input di un neurone è la somma pesata dei weight in ingresso per il segnale emesso.

$$
I = w_1 x_1 + w_2 x_2 + \dots + w_n x_n
$$

Fanno anche degli esempi interessanti... usando questo modello posso fare delle porte logiche e se ho delle porte
logiche posso costruire qualcosa che elabori informazioni.

Un nodo che implementa l'OR ha due input, soglia 0.5 e pesi sui link di 1.
Se invece alzo la soglia a 1.5, ho realizzato un AND (l'unico modo per superare la soglia è che la somma sia 2, che
avviene solo quando entrambi sono a 1).

Se metto un peso negativo ho un NOT su un input.

Questo è il primo punto di contatto tra cervello biologico e elaborazione elettronica.

Si può realizzare anche l'XOR, ma bisogna aggiungere un livello. Mentre McCullogh e Pitts si stanno occupando solo del
percettrone semplice.
